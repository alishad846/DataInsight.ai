{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "load_libraries",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "TensorFlow version: 2.13.1\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_absolute_error, r2_score\n",
                "\n",
                "print('TensorFlow version:', tf.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "load_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data loaded, converted, and scaled successfully.\n",
                        "X_train shape: (1106, 53)\n"
                    ]
                }
            ],
            "source": [
                "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
                "X_test  = pd.read_csv('../data/processed/X_test.csv')\n",
                "y_train = pd.read_csv('../data/processed/y_train.csv').values.ravel()\n",
                "y_test  = pd.read_csv('../data/processed/y_test.csv').values.ravel()\n",
                "\n",
                "# Convert to float32\n",
                "X_train = X_train.astype('float32')\n",
                "X_test  = X_test.astype('float32')\n",
                "y_train = y_train.astype('float32')\n",
                "y_test  = y_test.astype('float32')\n",
                "\n",
                "# ── Feature Scaling ──────────────────────────────────────────────────────────\n",
                "# StandardScaler eliminates gradient instability caused by different feature\n",
                "# magnitudes — the biggest single cause of val_loss spikes in the baseline.\n",
                "scaler_X = StandardScaler()\n",
                "X_train_scaled = scaler_X.fit_transform(X_train)   # fit ONLY on train data\n",
                "X_test_scaled  = scaler_X.transform(X_test)\n",
                "\n",
                "# Scale the target too so the network learns smaller numbers\n",
                "scaler_y = StandardScaler()\n",
                "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
                "y_test_scaled  = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
                "\n",
                "print('Data loaded, converted, and scaled successfully.')\n",
                "print('X_train shape:', X_train_scaled.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "build_model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: \"sequential_1\"\n",
                        "_________________________________________________________________\n",
                        " Layer (type)                Output Shape              Param #   \n",
                        "=================================================================\n",
                        " dense_5 (Dense)             (None, 256)               13824     \n",
                        "                                                                 \n",
                        " batch_normalization_4 (Bat  (None, 256)               1024      \n",
                        " chNormalization)                                                \n",
                        "                                                                 \n",
                        " dropout_3 (Dropout)         (None, 256)               0         \n",
                        "                                                                 \n",
                        " dense_6 (Dense)             (None, 128)               32896     \n",
                        "                                                                 \n",
                        " batch_normalization_5 (Bat  (None, 128)               512       \n",
                        " chNormalization)                                                \n",
                        "                                                                 \n",
                        " dropout_4 (Dropout)         (None, 128)               0         \n",
                        "                                                                 \n",
                        " dense_7 (Dense)             (None, 64)                8256      \n",
                        "                                                                 \n",
                        " batch_normalization_6 (Bat  (None, 64)                256       \n",
                        " chNormalization)                                                \n",
                        "                                                                 \n",
                        " dropout_5 (Dropout)         (None, 64)                0         \n",
                        "                                                                 \n",
                        " dense_8 (Dense)             (None, 32)                2080      \n",
                        "                                                                 \n",
                        " batch_normalization_7 (Bat  (None, 32)                128       \n",
                        " chNormalization)                                                \n",
                        "                                                                 \n",
                        " dense_9 (Dense)             (None, 1)                 33        \n",
                        "                                                                 \n",
                        "=================================================================\n",
                        "Total params: 59009 (230.50 KB)\n",
                        "Trainable params: 58049 (226.75 KB)\n",
                        "Non-trainable params: 960 (3.75 KB)\n",
                        "_________________________________________________________________\n"
                    ]
                }
            ],
            "source": [
                "# ── Build Improved Model ─────────────────────────────────────────────────────\n",
                "# Improvements over baseline:\n",
                "#   • Deeper network  : 256 → 128 → 64 → 32 → 1\n",
                "#   • BatchNormalization: stabilises activations, speeds convergence\n",
                "#   • Dropout(0.3)    : reduces overfitting on small dataset (~1 106 rows)\n",
                "\n",
                "def build_improved_model(input_dim):\n",
                "    model = Sequential([\n",
                "        # Block 1\n",
                "        Dense(256, activation='relu', input_shape=(input_dim,)),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.3),\n",
                "\n",
                "        # Block 2\n",
                "        Dense(128, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.3),\n",
                "\n",
                "        # Block 3\n",
                "        Dense(64, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "        Dropout(0.2),\n",
                "\n",
                "        # Block 4\n",
                "        Dense(32, activation='relu'),\n",
                "        BatchNormalization(),\n",
                "\n",
                "        # Output  (linear activation for regression)\n",
                "        Dense(1)\n",
                "    ])\n",
                "    return model\n",
                "\n",
                "model = build_improved_model(X_train_scaled.shape[1])\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "compile_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── Compile Model ─────────────────────────────────────────────────────────────\n",
                "model.compile(\n",
                "    optimizer=Adam(learning_rate=0.001),\n",
                "    loss='mean_squared_error',\n",
                "    metrics=['mean_absolute_error']\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "train_model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/300\n",
                        "35/35 [==============================] - 1s 7ms/step - loss: 1.7471 - mean_absolute_error: 1.0094 - val_loss: 0.5942 - val_mean_absolute_error: 0.6130 - lr: 0.0010\n",
                        "Epoch 2/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.9348 - mean_absolute_error: 0.7541 - val_loss: 0.5372 - val_mean_absolute_error: 0.5832 - lr: 0.0010\n",
                        "Epoch 3/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.6419 - mean_absolute_error: 0.6333 - val_loss: 0.4746 - val_mean_absolute_error: 0.5541 - lr: 0.0010\n",
                        "Epoch 4/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.5617 - mean_absolute_error: 0.5883 - val_loss: 0.3630 - val_mean_absolute_error: 0.4831 - lr: 0.0010\n",
                        "Epoch 5/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.5100 - mean_absolute_error: 0.5570 - val_loss: 0.2915 - val_mean_absolute_error: 0.4251 - lr: 0.0010\n",
                        "Epoch 6/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.4510 - mean_absolute_error: 0.5223 - val_loss: 0.2779 - val_mean_absolute_error: 0.4056 - lr: 0.0010\n",
                        "Epoch 7/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.4225 - mean_absolute_error: 0.5076 - val_loss: 0.2833 - val_mean_absolute_error: 0.4030 - lr: 0.0010\n",
                        "Epoch 8/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.3457 - mean_absolute_error: 0.4632 - val_loss: 0.2878 - val_mean_absolute_error: 0.3938 - lr: 0.0010\n",
                        "Epoch 9/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.3194 - mean_absolute_error: 0.4425 - val_loss: 0.2784 - val_mean_absolute_error: 0.3770 - lr: 0.0010\n",
                        "Epoch 10/300\n",
                        "35/35 [==============================] - 0s 4ms/step - loss: 0.3296 - mean_absolute_error: 0.4545 - val_loss: 0.2888 - val_mean_absolute_error: 0.3755 - lr: 0.0010\n",
                        "Epoch 11/300\n",
                        "35/35 [==============================] - 0s 4ms/step - loss: 0.3022 - mean_absolute_error: 0.4317 - val_loss: 0.2588 - val_mean_absolute_error: 0.3611 - lr: 0.0010\n",
                        "Epoch 12/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2803 - mean_absolute_error: 0.4158 - val_loss: 0.2643 - val_mean_absolute_error: 0.3543 - lr: 0.0010\n",
                        "Epoch 13/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.3092 - mean_absolute_error: 0.4342 - val_loss: 0.3077 - val_mean_absolute_error: 0.3565 - lr: 0.0010\n",
                        "Epoch 14/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2663 - mean_absolute_error: 0.3995 - val_loss: 0.2907 - val_mean_absolute_error: 0.3496 - lr: 0.0010\n",
                        "Epoch 15/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2318 - mean_absolute_error: 0.3851 - val_loss: 0.2948 - val_mean_absolute_error: 0.3482 - lr: 0.0010\n",
                        "Epoch 16/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2331 - mean_absolute_error: 0.3787 - val_loss: 0.2688 - val_mean_absolute_error: 0.3414 - lr: 0.0010\n",
                        "Epoch 17/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2245 - mean_absolute_error: 0.3712 - val_loss: 0.2716 - val_mean_absolute_error: 0.3371 - lr: 0.0010\n",
                        "Epoch 18/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2322 - mean_absolute_error: 0.3803 - val_loss: 0.2437 - val_mean_absolute_error: 0.3329 - lr: 0.0010\n",
                        "Epoch 19/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2495 - mean_absolute_error: 0.3935 - val_loss: 0.2574 - val_mean_absolute_error: 0.3266 - lr: 0.0010\n",
                        "Epoch 20/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2244 - mean_absolute_error: 0.3703 - val_loss: 0.2499 - val_mean_absolute_error: 0.3253 - lr: 0.0010\n",
                        "Epoch 21/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2129 - mean_absolute_error: 0.3682 - val_loss: 0.2375 - val_mean_absolute_error: 0.3200 - lr: 0.0010\n",
                        "Epoch 22/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2106 - mean_absolute_error: 0.3641 - val_loss: 0.2444 - val_mean_absolute_error: 0.3308 - lr: 0.0010\n",
                        "Epoch 23/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2044 - mean_absolute_error: 0.3587 - val_loss: 0.2294 - val_mean_absolute_error: 0.3204 - lr: 0.0010\n",
                        "Epoch 24/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1952 - mean_absolute_error: 0.3486 - val_loss: 0.2238 - val_mean_absolute_error: 0.3140 - lr: 0.0010\n",
                        "Epoch 25/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1974 - mean_absolute_error: 0.3485 - val_loss: 0.2273 - val_mean_absolute_error: 0.3136 - lr: 0.0010\n",
                        "Epoch 26/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1925 - mean_absolute_error: 0.3433 - val_loss: 0.2257 - val_mean_absolute_error: 0.3062 - lr: 0.0010\n",
                        "Epoch 27/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.2058 - mean_absolute_error: 0.3563 - val_loss: 0.2296 - val_mean_absolute_error: 0.3103 - lr: 0.0010\n",
                        "Epoch 28/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1837 - mean_absolute_error: 0.3417 - val_loss: 0.2239 - val_mean_absolute_error: 0.3062 - lr: 0.0010\n",
                        "Epoch 29/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1780 - mean_absolute_error: 0.3352 - val_loss: 0.2163 - val_mean_absolute_error: 0.3068 - lr: 0.0010\n",
                        "Epoch 30/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1846 - mean_absolute_error: 0.3400 - val_loss: 0.2181 - val_mean_absolute_error: 0.3093 - lr: 0.0010\n",
                        "Epoch 31/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1950 - mean_absolute_error: 0.3424 - val_loss: 0.2076 - val_mean_absolute_error: 0.3132 - lr: 0.0010\n",
                        "Epoch 32/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1853 - mean_absolute_error: 0.3412 - val_loss: 0.2072 - val_mean_absolute_error: 0.3076 - lr: 0.0010\n",
                        "Epoch 33/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1826 - mean_absolute_error: 0.3349 - val_loss: 0.2042 - val_mean_absolute_error: 0.3173 - lr: 0.0010\n",
                        "Epoch 34/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1874 - mean_absolute_error: 0.3361 - val_loss: 0.2129 - val_mean_absolute_error: 0.3099 - lr: 0.0010\n",
                        "Epoch 35/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1835 - mean_absolute_error: 0.3417 - val_loss: 0.2011 - val_mean_absolute_error: 0.3171 - lr: 0.0010\n",
                        "Epoch 36/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1652 - mean_absolute_error: 0.3204 - val_loss: 0.2034 - val_mean_absolute_error: 0.3075 - lr: 0.0010\n",
                        "Epoch 37/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1924 - mean_absolute_error: 0.3455 - val_loss: 0.1970 - val_mean_absolute_error: 0.3036 - lr: 0.0010\n",
                        "Epoch 38/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1671 - mean_absolute_error: 0.3192 - val_loss: 0.1993 - val_mean_absolute_error: 0.3057 - lr: 0.0010\n",
                        "Epoch 39/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1765 - mean_absolute_error: 0.3317 - val_loss: 0.2006 - val_mean_absolute_error: 0.3021 - lr: 0.0010\n",
                        "Epoch 40/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1686 - mean_absolute_error: 0.3238 - val_loss: 0.2009 - val_mean_absolute_error: 0.3099 - lr: 0.0010\n",
                        "Epoch 41/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1635 - mean_absolute_error: 0.3208 - val_loss: 0.2066 - val_mean_absolute_error: 0.3040 - lr: 0.0010\n",
                        "Epoch 42/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1710 - mean_absolute_error: 0.3294 - val_loss: 0.2078 - val_mean_absolute_error: 0.3168 - lr: 0.0010\n",
                        "Epoch 43/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1720 - mean_absolute_error: 0.3269 - val_loss: 0.2046 - val_mean_absolute_error: 0.3087 - lr: 0.0010\n",
                        "Epoch 44/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1801 - mean_absolute_error: 0.3389 - val_loss: 0.1987 - val_mean_absolute_error: 0.3154 - lr: 0.0010\n",
                        "Epoch 45/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1811 - mean_absolute_error: 0.3383 - val_loss: 0.1922 - val_mean_absolute_error: 0.3103 - lr: 0.0010\n",
                        "Epoch 46/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1696 - mean_absolute_error: 0.3251 - val_loss: 0.1838 - val_mean_absolute_error: 0.3030 - lr: 0.0010\n",
                        "Epoch 47/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1724 - mean_absolute_error: 0.3273 - val_loss: 0.1837 - val_mean_absolute_error: 0.3080 - lr: 0.0010\n",
                        "Epoch 48/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1715 - mean_absolute_error: 0.3228 - val_loss: 0.1850 - val_mean_absolute_error: 0.3134 - lr: 0.0010\n",
                        "Epoch 49/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1527 - mean_absolute_error: 0.3105 - val_loss: 0.1817 - val_mean_absolute_error: 0.3014 - lr: 0.0010\n",
                        "Epoch 50/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1612 - mean_absolute_error: 0.3125 - val_loss: 0.1810 - val_mean_absolute_error: 0.3089 - lr: 0.0010\n",
                        "Epoch 51/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1651 - mean_absolute_error: 0.3221 - val_loss: 0.1789 - val_mean_absolute_error: 0.3017 - lr: 0.0010\n",
                        "Epoch 52/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1573 - mean_absolute_error: 0.3132 - val_loss: 0.1804 - val_mean_absolute_error: 0.3018 - lr: 0.0010\n",
                        "Epoch 53/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1427 - mean_absolute_error: 0.2975 - val_loss: 0.1696 - val_mean_absolute_error: 0.2998 - lr: 0.0010\n",
                        "Epoch 54/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1560 - mean_absolute_error: 0.3133 - val_loss: 0.1680 - val_mean_absolute_error: 0.3012 - lr: 0.0010\n",
                        "Epoch 55/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1564 - mean_absolute_error: 0.3115 - val_loss: 0.1688 - val_mean_absolute_error: 0.3053 - lr: 0.0010\n",
                        "Epoch 56/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1622 - mean_absolute_error: 0.3142 - val_loss: 0.1668 - val_mean_absolute_error: 0.2991 - lr: 0.0010\n",
                        "Epoch 57/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1511 - mean_absolute_error: 0.3051 - val_loss: 0.1638 - val_mean_absolute_error: 0.2961 - lr: 0.0010\n",
                        "Epoch 58/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1544 - mean_absolute_error: 0.3133 - val_loss: 0.1693 - val_mean_absolute_error: 0.2997 - lr: 0.0010\n",
                        "Epoch 59/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1377 - mean_absolute_error: 0.2924 - val_loss: 0.1646 - val_mean_absolute_error: 0.2982 - lr: 0.0010\n",
                        "Epoch 60/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1474 - mean_absolute_error: 0.2977 - val_loss: 0.1582 - val_mean_absolute_error: 0.2894 - lr: 0.0010\n",
                        "Epoch 61/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1413 - mean_absolute_error: 0.2938 - val_loss: 0.1579 - val_mean_absolute_error: 0.2946 - lr: 0.0010\n",
                        "Epoch 62/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1523 - mean_absolute_error: 0.3068 - val_loss: 0.1533 - val_mean_absolute_error: 0.2860 - lr: 0.0010\n",
                        "Epoch 63/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1475 - mean_absolute_error: 0.3049 - val_loss: 0.1532 - val_mean_absolute_error: 0.2881 - lr: 0.0010\n",
                        "Epoch 64/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1425 - mean_absolute_error: 0.2945 - val_loss: 0.1453 - val_mean_absolute_error: 0.2816 - lr: 0.0010\n",
                        "Epoch 65/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1429 - mean_absolute_error: 0.2992 - val_loss: 0.1428 - val_mean_absolute_error: 0.2804 - lr: 0.0010\n",
                        "Epoch 66/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1441 - mean_absolute_error: 0.3008 - val_loss: 0.1486 - val_mean_absolute_error: 0.2846 - lr: 0.0010\n",
                        "Epoch 67/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1706 - mean_absolute_error: 0.3224 - val_loss: 0.1553 - val_mean_absolute_error: 0.2920 - lr: 0.0010\n",
                        "Epoch 68/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1442 - mean_absolute_error: 0.3072 - val_loss: 0.1485 - val_mean_absolute_error: 0.2847 - lr: 0.0010\n",
                        "Epoch 69/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1439 - mean_absolute_error: 0.3013 - val_loss: 0.1517 - val_mean_absolute_error: 0.2903 - lr: 0.0010\n",
                        "Epoch 70/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1365 - mean_absolute_error: 0.2905 - val_loss: 0.1535 - val_mean_absolute_error: 0.2772 - lr: 0.0010\n",
                        "Epoch 71/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1425 - mean_absolute_error: 0.2988 - val_loss: 0.1619 - val_mean_absolute_error: 0.2876 - lr: 0.0010\n",
                        "Epoch 72/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1369 - mean_absolute_error: 0.2872 - val_loss: 0.1679 - val_mean_absolute_error: 0.2861 - lr: 0.0010\n",
                        "Epoch 73/300\n",
                        "23/35 [==================>...........] - ETA: 0s - loss: 0.1540 - mean_absolute_error: 0.3093\n",
                        "Epoch 73: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1582 - mean_absolute_error: 0.3138 - val_loss: 0.1603 - val_mean_absolute_error: 0.2880 - lr: 0.0010\n",
                        "Epoch 74/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1495 - mean_absolute_error: 0.3031 - val_loss: 0.1604 - val_mean_absolute_error: 0.2826 - lr: 5.0000e-04\n",
                        "Epoch 75/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1443 - mean_absolute_error: 0.3010 - val_loss: 0.1570 - val_mean_absolute_error: 0.2828 - lr: 5.0000e-04\n",
                        "Epoch 76/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1355 - mean_absolute_error: 0.2921 - val_loss: 0.1516 - val_mean_absolute_error: 0.2743 - lr: 5.0000e-04\n",
                        "Epoch 77/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1484 - mean_absolute_error: 0.2997 - val_loss: 0.1521 - val_mean_absolute_error: 0.2816 - lr: 5.0000e-04\n",
                        "Epoch 78/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1415 - mean_absolute_error: 0.2951 - val_loss: 0.1461 - val_mean_absolute_error: 0.2776 - lr: 5.0000e-04\n",
                        "Epoch 79/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1408 - mean_absolute_error: 0.2987 - val_loss: 0.1442 - val_mean_absolute_error: 0.2770 - lr: 5.0000e-04\n",
                        "Epoch 80/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1287 - mean_absolute_error: 0.2800 - val_loss: 0.1491 - val_mean_absolute_error: 0.2779 - lr: 5.0000e-04\n",
                        "Epoch 81/300\n",
                        "21/35 [=================>............] - ETA: 0s - loss: 0.1307 - mean_absolute_error: 0.2838\n",
                        "Epoch 81: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1421 - mean_absolute_error: 0.3004 - val_loss: 0.1479 - val_mean_absolute_error: 0.2802 - lr: 5.0000e-04\n",
                        "Epoch 82/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1419 - mean_absolute_error: 0.3001 - val_loss: 0.1463 - val_mean_absolute_error: 0.2796 - lr: 2.5000e-04\n",
                        "Epoch 83/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1374 - mean_absolute_error: 0.2948 - val_loss: 0.1482 - val_mean_absolute_error: 0.2812 - lr: 2.5000e-04\n",
                        "Epoch 84/300\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1320 - mean_absolute_error: 0.2813 - val_loss: 0.1493 - val_mean_absolute_error: 0.2793 - lr: 2.5000e-04\n",
                        "Epoch 85/300\n",
                        "22/35 [=================>............] - ETA: 0s - loss: 0.1393 - mean_absolute_error: 0.2970Restoring model weights from the end of the best epoch: 65.\n",
                        "35/35 [==============================] - 0s 3ms/step - loss: 0.1428 - mean_absolute_error: 0.3021 - val_loss: 0.1490 - val_mean_absolute_error: 0.2799 - lr: 2.5000e-04\n",
                        "Epoch 85: early stopping\n"
                    ]
                }
            ],
            "source": [
                "# ── Callbacks ─────────────────────────────────────────────────────────────────\n",
                "early_stop = EarlyStopping(\n",
                "    monitor='val_loss',\n",
                "    patience=20,                  # wait 20 epochs with no improvement\n",
                "    restore_best_weights=True,    # roll back to best weights automatically\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "reduce_lr = ReduceLROnPlateau(\n",
                "    monitor='val_loss',\n",
                "    factor=0.5,                   # halve LR when plateau is detected\n",
                "    patience=8,\n",
                "    min_lr=1e-6,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# ── Train ─────────────────────────────────────────────────────────────────────\n",
                "history = model.fit(\n",
                "    X_train_scaled, y_train_scaled,\n",
                "    epochs=300,                   # early stopping handles actual termination\n",
                "    batch_size=32,\n",
                "    validation_data=(X_test_scaled, y_test_scaled),\n",
                "    callbacks=[early_stop, reduce_lr],\n",
                "    verbose=1\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "evaluate_model",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "7/7 [==============================] - 0s 968us/step\n",
                        "========================================\n",
                        "  Baseline MAE : ~0.6639\n",
                        "  Improved MAE : 0.1751\n",
                        "  R² Score     : 0.8544  (1.0 = perfect)\n",
                        "========================================\n",
                        " MAE improved by 73.6%\n"
                    ]
                }
            ],
            "source": [
                "# ── Evaluate Model ────────────────────────────────────────────────────────────\n",
                "# Predict on scaled test data, then invert scaling for real-world metrics\n",
                "y_pred_scaled = model.predict(X_test_scaled).ravel()\n",
                "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
                "\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "r2  = r2_score(y_test, y_pred)\n",
                "\n",
                "print('=' * 40)\n",
                "print(f'  Baseline MAE : ~0.6639')\n",
                "print(f'  Improved MAE : {mae:.4f}')\n",
                "print(f'  R² Score     : {r2:.4f}  (1.0 = perfect)')\n",
                "print('=' * 40)\n",
                "\n",
                "if mae < 0.6639:\n",
                "    print(f' MAE improved by {((0.6639 - mae) / 0.6639 * 100):.1f}%')\n",
                "else:\n",
                "    print(' MAE did not improve — consider re-running (random seed variation)')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
